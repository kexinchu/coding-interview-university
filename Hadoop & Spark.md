## Hadoop & Spark

### hadoop 与 Spark 的关系
从以下几方面来比较Apache Spark与Apache Hadoop.
1.解决问题的层面不同
首先，Apache Spark与Apache Hadoop两者都是大数据框架，但是各自存在的目的不尽相同。Hadoop实质上更多是一个分布式数据基础设施，它将巨大的数据集分派到一个由普通计算机组成的集群中的多个节点进行存储，意味着我们不需要购买和维护昂贵的服务器硬件。
同时，Hadoop还会索引和跟踪这些数据、让大数据处理和分析效率达到前所未有的高度。Spark则是一个用来对那些分布式存储的大数据进行处理的工具，它并不会进行分布式数据的存储。
2.数据处理方式不同
Spark因为其处理的方式不一样，会比Hadoop MapRecue快上很多。Hadoop MapRecue是分步对数据进行处理的，先从集群中读取数据，进行一次处理，将结果写到集群，从集群中读取更新后的数据，再进行下一次的处理，将结果写到集群。
而反观Spark，它会在内在中以接近“实时”的时间完成所有的数据分析，集群中读取数据，完成所有必须的分析处理，将结果写回集群，整个计算过程就完成了。所以Spark的批处理速度比Hadoop MapRecue快近10倍，内存中的数据分析速度则快近100倍。
如果需要处理的数据和结果需求大部分情况下是静态的，而且你也不耐心等待批处理完成的话，Hadoop MapRecue的处理方式也是完成可以接受的。但如果你需要对流数据进行分析，比如那些来自工厂的传感器收集回来的数据，又或者说你的应用是需要多重数据处理的，那你也许更应该使用Spark进行处理。
大部分机器学习算法都是需要多重数据处理的。此外，通常会用到Spark的应用场景有以下几个方面：实时的市场活动、在线产品推荐、网络安全分析、机器日志监控等。
3.容灾处理
两者的空难恢复方式迥异，但是都不错。因为Hadoop将每次处理后的数据写入到磁盘上，所以其天生就能很有弹性地对系统错误进行处理。Spark的数据对象存储在分布于数据集群中的叫作弹性分布式数据集（RDD，Resilient Distributed Dataset）中。这些数据对象即可以放在内存中，也可以放在磁盘中，所以RDD同样也可以提供完整的空难恢复功能。
4.两者各为互补
Hadoop除了提供我们熟悉的HDFS分布式数据存储功能，还提供了MapRecue的数据处理功能，YARN的资源调试系统。所以这里我们完成抛开Spark，使用Hadoop自身的MapRecue来完成对数据的处理。
相反，Spark也不是非要依附在Hadoop身上才能生存。但如上所述，毕竟它没有提供文件管理系统，所以，它必须和其他的分布式文件系统进行集成才能运作。这里我们可以选择Hadoop的HDFS，也可以选择其他的，比如Red Hat GlusterFS。当需要外部的资源调度系统来支持时，Spark可以跑在YARN上，也可以跑在Apache Mesos上，当然也可以用Standalone模式。但Spark默认来说还是被用在Hadoop上面的，毕境，大家都认为它们的结合是最好的。


### Spark
Spark 框架包括：
    Spark Core 是该平台的基础
    用于交互式查询的 Spark SQL
    用于实时分析的 Spark Streaming
    用于机器学习的 Spark MLlib
    用于图形处理的 Spark GraphX

Spark是一个快速和通用的集群计算系统。特别：

快速 Spack具有支持循环数据流和内存计算的先进的DAG执行引擎，所以比Hadoop MapRecue在内存计算上快100倍，在硬盘计算上快10倍。
易于使用 Spark提供了Java，Scala，Python和R等语言的高级API，可以用于快速开发相关语言应用。Spark提供了超过80个高级的操作，可以轻松构建并行应用程序。
全面 Spark提供了Spark SQL，机器学习的MLlib，进行图形处理的GraphX，以及Spark Streaming等库。你可以在同一应用程序无缝地合并这些库。
到处运行 可以standalone cluster mode运行EC2、Hadoop YARN、或者Apache Mesos中。可以访问HDFS、Cassandra、HBase、Hive、Tachyon，以及任意的Hadoop数据源。

MapRecue
在过去的20年里，互联网产生了大量的数据，比如爬虫文档、Web讲求日志等；也包括了计算各种类型的派生数据，比如，倒排索引、Web文档的图结构的各种表示、每台主机页面数量概要、每天被请求数量最多的集合，等等。很多这样的计算在概念上很容易理解的。然而，当输入的数据量很大时，这些计算必须要被分割到成百上千的机器上才有可能在可以接受的时间内完成。怎样来实现并行计算？如何分发数据？如何进行错误处理？所有这些问题综合在一起，使得原来很简洁的计算，因为要大量的复杂代码来处理这些问题，而变得让人难以处理。 Google公司为了应对大数据的处理，内部已经实现了数据以百计的为专门目的而写的计算程序，其中MapRecue就是其著名的计算框架之王，与GFS、Bigtable一起被称为Google技术的“三宝”。

MapRecue简介
MapRecue是一个编程模型，用于大规模数据集（TB级）的并行运算。有关MapRecue的论文介绍，最早可以追溯到由Google的Jeffrey Dean和Sanjay Ghemawat发表在2004年OSDI（USENIX Symposium on Operationg Systems Design and Implementation）的《MapRecue：Simplified Data Processing on LargeClusters》。这篇文章描述了Google如何分割、处理、整合他们令人难以置信的大数据集。读者有兴趣可以在线阅读该论文https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf。 随后，开源软件先驱Doug Cutting等人受到该论文的启发，开始尝试实现MapRecue计算框架，并将它与NDFS（Nutch Distributed File System）结合，用以支持Nutch引擎的主要算法。由于NDFS与MapRecue在Nutch引擎中有着良好的应用，所以它们于2006年2月被分离出来，成为一套完整而独立的软件，并命名为Hadoop。 MapRecue程序模型应用成功要归功于以下几个方面。首先，由于该模型隐藏了并行、容错、本地优化以及负载平衡的细节，所以即便是那些没有并行和分布式系统经验的程序员也易于使用该模型。其次MapRecue计算可以很容易地表达大数据的各种问题。比如，MapRecue用于为Google的网页搜索服务生成数据，用于排序，用于数据挖掘，用于机器学习以及其他许多系统。再次，MapRecue的实现符合“由数千机器组成的大集群”的尺度，有效地利用了机器资源，所以非常适合解决大型计算问题。

MapRecue的编程模型
MapRecue是一个用于大规模数据集（TB级）并行运算的编程模型，其基本原理就是将大的数据分成小块逐个分析，最后再将提取出来的数据汇总分析，最终获得我们想要的内容。从名字可以看出，“Map(映射)”和“Reduce（归纳）”是MapRecue模型的核心，其灵感来源于函数式语言（比如Lisp）中的内置函数map和reduce：用户通过定义一个Map函数，处理key/value（键值对）以生成一个中间key/value集合，MapRecue库将所有拥有相同的key(key I)的中间状态key合并起来传递到Redure职数；一个叫作Reduce的函数用以合并所有先前Map过后的有相同key（Key I）的中间量。map(k1,v1) -> list<k2,v2)reduce(k2,list(v2)) -> list(k3,v3)但上面的定义显然还是过于抽象。现实世界中的许多任务在这个模型中得到了很好的表达。Shekhar Gulati就在他的博客里《How I explained MapReduce to my Wife?》举了一个辣椒酱制作过程的例子，来形象地描述MapRecue的原理，如下所述。1.MapRecue制作辣椒酱的过程辣椒酱制作的过程是这样的，先取一个洋葱，把它切碎，然后拌入盐和水，最后放进混合研磨机研磨。这样就能得到洋葱辣椒酱了。 现在，假设你想用薄荷、洋葱、番茄、辣椒、大蒜弄一瓶混合辣椒酱。你会怎么做呢？你会取薄荷叶一撮，洋葱一个，番茄一个，辣椒一根，大蒜一根，切碎后加入适量的盐和水，再放入混合研磨机里研磨，这样你就可以得到一瓶混合辣椒酱了。 现在把MapRecue的概念应用到食谱上，Map和Reduce其实就是两种操作。 Map：把洋葱、番茄、辣椒和大蒜切磋，是各自作用在这些物体上的一个Map操作。所以你给Map一个洋葱，Map就会把洋葱切碎。同样地，你把辣椒、大蒜和番茄一一地拿给Map，你也会得到各种碎块。所以，当你在切像洋葱这样的蔬菜时，你执行的就是一个Map操作。Map操作适用于每一种蔬菜，它会相应地生产出一种或多种碎块，在我们的例子中生产的是蔬菜块。在Map操作中可能会出现有个洋葱坏掉了的情况，你只要把洋葱丢了就行了。所以，如果出现坏洋葱了，Map操作就会过滤掉这个坏洋葱而不会生产出任何的坏洋葱块。 Reduce：在这一阶段，你将各种蔬菜都放入研磨机时在进行研磨，你就可以得到一瓶辣椒酱了。这意味要制成一瓶辣椒酱，你得研磨所有的原料。因此，研磨机通常将Map操作的蔬菜聚焦在了一起。 当然上面内容只是MapRecue的一部分，MapRecue的强大在于分布式计算。假设你每天需要生产10000瓶辣椒酱，你会怎么办？这个时候你就不得不雇佣更多的人和研磨机来完成这项工作了，你需要几个人一起切蔬菜。每个人都要处理满满一袋子的蔬菜，而每一个人都相当于在执行一个简单的Map操作。每一个人都将不断地从袋子里拿出蔬菜来，并且每次只对一种蔬菜进行处理，也就是将它们切碎，直到袋子空了为止。这样，当所有的工人都切完以后，工作台（每个人工作的地方）上就有了洋葱块、番茄块和蒜蓉，等等。 MapRecue将所有输出的蔬菜都搅拌在了一起，这些蔬菜都在以key为基础的Map操作下产生的。搅拌将自动完成，你可以假设key是一种原料的名字，你像洋葱一样。所以全部的洋葱key都搅拌在一起，并转移到研磨洋葱的研磨器里。这样，你就能得到洋葱辣椒酱了。同样地，所有的番茄也会被转移地标记着番茄的研磨器里，并制造出番茄辣椒酱。

hadoop和Spark是两种不同的大数据处理框架，他们的组件都非常多，往往也不容易学，我把他们两者整理在一幅图中，给大家一个全貌的感觉。

<img src="https://github.com/kexinchu/coding-interview-university/blob/main/pictures/relations-hadoop-spark.jpeg" width="450px">    

以上这些大数据组件是日常大数据工作中经常会碰到的，每个组件大概的功能，已经在图中做了标识。下面，针对这幅图我给大家两点重要提示：
a.蓝色部分，是Hadoop生态系统组件，黄色部分是Spark生态组件，虽然他们是两种不同的大数据处理框架，但它们不是互斥的，Spark与hadoop 中的MapReduce是一种相互共生的关系。Hadoop提供了Spark许多没有的功能，比如分布式文件系统，而Spark 提供了实时内存计算，速度非常快。有一点大家要注意，Spark并不是一定要依附于Hadoop才能生存，除了Hadoop的HDFS，还可以基于其他的云平台，当然啦，大家一致认为Spark与Hadoop配合默契最好摆了。

b.技术趋势：Spark在崛起，hadoop和Storm中的一些组件在消退。大家在学习使用相关技术的时候，记得与时俱进掌握好新的趋势、新的替代技术，以保持自己的职业竞争力。
HSQL未来可能会被Spark SQL替代，现在很多企业都是HIVE SQL和Spark SQL两种工具共存，当Spark SQL逐步成熟的时候，就有可能替换HSQL；
MapReduce也有可能被Spark 替换，趋势是这样，但目前Spark还不够成熟稳定，还有比较长的路要走；
Hadoop中的算法库Mahout正被Spark中的算法库MLib所替代，为了不落后，大家注意去学习Mlib算法库；
Storm会被Spark Streaming替换吗?在这里，Storm虽然不是hadoop生态中的一员，但我仍然想把它放在一起做过比较。由于Spark和hadoop天衣无缝的结合，Spark在逐步的走向成熟和稳定，其生态组件也在逐步的完善，是冉冉升起的新星，我相信Storm会逐步被挤压而走向衰退。



