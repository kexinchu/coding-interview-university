## å¿…è€ƒçŸ¥è¯†
- æŸå¤±å‡½æ•°ã€æ¢¯åº¦ä¸‹é™ã€å­¦ä¹ ç‡ç­–ç•¥
- BatchNorm / LayerNorm æœºåˆ¶
- Attention åŸç†ï¼ˆQKVï¼Œç¼©æ”¾ç‚¹ç§¯ï¼‰
- Transformer ç»“æ„
    - Multi-head
    - Residual
    - FFN
    - Pre-norm vs Post-norm
- ä¼˜åŒ–å™¨ï¼šAdam / AdamW / Lion
- å¸¸è§ regularizationï¼šDropoutã€weight decay

## ğŸ¯ é«˜é¢‘é—®é¢˜
- ä¸ºä»€ä¹ˆ self-attention æ˜¯ O(NÂ²)ï¼Ÿ
- ä¸ºä»€ä¹ˆ LayerNorm é€‚åˆ Transformerï¼Ÿ
- GELU ä¸ºä»€ä¹ˆæ¯” ReLU å¥½ï¼Ÿ
- è®²ä¸€ä¸‹ KV-Cache æ˜¯ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆèƒ½åŠ é€Ÿï¼Ÿ
- RoPE çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ