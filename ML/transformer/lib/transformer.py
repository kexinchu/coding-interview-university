"""
åŸºäºpytorchçš„transformerå®ç°
study the details of transformer architecture
Scaled Dot-Product Attention & Multi-Head Attention

Attention(Q,K,V)=softmax(QK^T / sqrt(d_k) + mask)V

è§£é‡Šï¼š
QK^T: æ¯ä¸ª query å‘é‡å’Œæ‰€æœ‰ key åšç‚¹ç§¯ï¼Œç‚¹ç§¯å¤§ â†’ æ–¹å‘ç›¸è¿‘ â†’ ç›¸ä¼¼åº¦é«˜ â†’ è¯´æ˜ query æ›´â€œå…³æ³¨â€è¿™ä¸ª keyã€‚è¿™æ ·å°±å¾—åˆ°ä¸€ä¸ª ç›¸ä¼¼åº¦çŸ©é˜µï¼Œå½¢çŠ¶æ˜¯ [L_q, L_k]ã€‚
/sqrt(d_k): ç¼©æ”¾å› å­ï¼Œé˜²æ­¢ç‚¹ç§¯è¿‡å¤§ï¼Œå¯¼è‡´ softmax éå¸¸â€œå°–â€ï¼Œæ¢¯åº¦å®¹æ˜“æ¶ˆå¤±ã€‚ï¼ˆ=å½’ä¸€åŒ–ï¼‰
softmax: æŠŠç›¸ä¼¼åº¦å˜æˆä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼šæ¯ä¸ª query å¯¹æ‰€æœ‰ key çš„æ³¨æ„åŠ›æƒé‡å’Œä¸º 1ã€‚è¿™æ ·å¯ä»¥è§£é‡Šä¸ºâ€œå¯¹æ¯ä¸ª queryï¼Œæ³¨æ„åŠ›æ˜¯ä¸€ä¸ªåœ¨æ‰€æœ‰ token ä¸Šçš„åŠ æƒå¹³å‡â€ã€‚
*V: ç”¨æ³¨æ„åŠ›æƒé‡å¯¹ value å‘é‡åšåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°å¯¹ query çš„â€œä¸Šä¸‹æ–‡è¡¨ç¤ºâ€ã€‚
mask(å¯é€‰): å¯¹ padding çš„ä½ç½®ã€æˆ–å¯¹æœªæ¥ tokenï¼ˆåœ¨ decoder é‡Œï¼‰åšé®æŒ¡ï¼Œä½¿è¿™äº›ä½ç½®çš„æƒé‡å˜æˆ 0ã€‚å®ç°ä¸Šé€šå¸¸æŠŠ mask çš„ç¦æ­¢ä½ç½®åŠ ä¸Šä¸€ä¸ªå¾ˆå¤§çš„è´Ÿæ•°ï¼ˆä¾‹å¦‚ -1e9ï¼‰ï¼Œç»è¿‡ softmax å°±æ¥è¿‘ 0ã€‚
"""
import math
import torch
import torch.nn as nn



class ScaledDotProductAttention(nn.Module):
    def __inti__(self, dropout=0.1):
        super().__init__() ## è°ƒç”¨çˆ¶ç±»çš„å±æ€§å’Œæ–¹æ³•
        self.dropout = nn.Dropout(dropout)

        """
        dropout: ä¸¢å¼ƒæ¦‚ç‡ p
        self.dropout(input: Tensot, p, training=self.training, inplace=False):
        training: æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹è¿›è¡Œä¸¢å¼ƒ
        _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
        è·¯ç”±åˆ° at::native::dropout (CPU C++ / CUDAå®ç°)
            aten/src/ATen/native/Dropout.cpp
            aten/src/ATen/native/cuda/Dropout.cu

        C++ version:
        Tensor dropout_cpu(const Tensor& input: Tensor, double p: float, bool train: bool) -> Tensor:
            if (!train) {
                return input;
            }
            // éšæœºbernoulliæ©ç 
            auto mask = at::bernoulli(input.sizes(), 1 - p);
            auto output = input * mask;
            output = output / (1 - p); 
            return output;
        }
        """

    def forward(self, Q, K, V, mask=None):
        """
        QKV: [batch_size, n_heads, seq_len, d_k]
        mask: [batch_size, 1, 1, seq_len]
        """
        d_k = q.shape[-1]

        # calculate dot
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        """
        torch.matmul å…¥å£åœ¨ Pythonï¼Œä½†çœŸæ­£çš„æ ¸å¿ƒå®ç°å…¨éƒ¨åœ¨ C++ã€‚
        æµç¨‹å¦‚ä¸‹ï¼š

        torch.matmul:
            â†“
        aten::matmul (C++ API)
            â†“
        Dispatcher æ ¹æ®å¼ é‡ç±»å‹é€‰æ‹©åç«¯ï¼š
            CPU â†’ MKL/OpenBLASï¼ˆgemm/gemv/...ï¼‰
            CUDA â†’ cuBLASï¼ˆcublasSgemm/cublasGemmEx/...ï¼‰
            GPU(batched) â†’ cuBLAS batched kernels
            broadcasting â†’ å†…éƒ¨ expand + reshape + è°ƒæ•´ strides


        å…³é”®ï¼š
        PyTorch å¹¶ä¸æ˜¯è‡ªå·±å†™çŸ©é˜µä¹˜ï¼Œè€Œæ˜¯é«˜åº¦ä¾èµ–æˆç†Ÿçš„ BLAS/cuBLAS åº“ã€‚
        è€Œä¸”matmul ä¸æ˜¯å•ä¸€æ“ä½œï¼Œè€Œæ˜¯ä¸€ç»„è§„åˆ™ï¼š

        è¾“å…¥å½¢çŠ¶	å®é™…æ“ä½œ
        2D x 2D	æ™®é€šçŸ©é˜µä¹˜ gemm
        1D x 1D	å†…ç§¯ dot
        1D x 2D	å‡ ä½•æ„ä¹‰ï¼šå‘é‡è§†ä¸º (1Ã—N)
        2D x 1D	å‡ ä½•æ„ä¹‰ï¼šå‘é‡è§†ä¸º (NÃ—1)
        >2D	broadcasting â†’ batched gemm
        """

        if mask is not None:
            # mask == 0 çš„ä½ç½®è¢«è®¾ç½®ä¸ºä¸€ä¸ªå¾ˆå°çš„å€¼
            scores = scores.masked_full(mask == 0, float('-inf'))

        # softmax attention
        attention = nn.funtional.softmax(scores, dim=-1)

        # dropout
        attention = self.dropout(attention)

        # mul V
        output = torch.matmul(attention, V)

        return output, attention
"""
å•å¤´æ³¨æ„åŠ›ï¼šæ¯ä¸ª token ç”¨ä¸€ä¸ª d_model ç»´çš„å‘é‡è¡¨ç¤ºï¼Œåšæ³¨æ„åŠ›ï¼›
å¤šå¤´æ³¨æ„åŠ›ï¼šæˆ‘ä»¬æŠŠ d_model åˆ‡åˆ†æˆ n_heads ä»½ï¼Œæ¯ä»½ç»´åº¦æ˜¯ d_k = d_model / n_headsï¼Œç„¶åï¼š
    - ç”¨ä¸åŒçš„çº¿æ€§å˜æ¢ï¼ŒæŠŠè¾“å…¥ ğ‘‹ æ˜ å°„åˆ°æ¯ä¸ªå¤´çš„ ğ‘„ğ‘–, ğ¾ğ‘–, ğ‘‰ğ‘–ï¼›
    - æ¯ä¸ªå¤´ç‹¬ç«‹åœ°åš Scaled Dot-Product Attentionï¼›
    - æŠŠæ‰€æœ‰å¤´çš„è¾“å‡ºåœ¨æœ€åä¸€ç»´æ‹¼èµ·æ¥ï¼ˆconcatenateï¼‰ï¼Œå†ä¹˜ä¸€ä¸ªçº¿æ€§å±‚ W_o åˆå¹¶æˆ d_model ç»´ã€‚
ä¸ºä»€ä¹ˆè¦å¤šå¤´ï¼Ÿ
    å¤šå¤´ = å¤šä¸ªå­ç©ºé—´ä¸Šçš„æ³¨æ„åŠ›ï¼Œä¸åŒçš„å¤´å¯ä»¥å­¦ä¹ åˆ°ä¸åŒçš„â€œå…³ç³»æ¨¡å¼â€ï¼šæœ‰çš„å¤´å…³æ³¨å±€éƒ¨é‚»è¿‘è¯ï¼Œæœ‰çš„å¤´æ•æ‰é•¿è·ç¦»ä¾èµ–ï¼Œæœ‰çš„å¤´å…³æ³¨è¯­æ³•ç»“æ„/å®ä½“ç­‰
"""
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads,dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"

        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        # q,k,v
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)

        self.w_o = nn.Linear(d_model, d_model)

        self.attention = ScaledDotProductAttention(dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, mask=None):
        """
        query, key, value: [batch_size, seq_len, d_model]
        mask: [batch_size, 1, 1, seq_len]
        """
        batch_size = query.size(0)

        # 1, çº¿æ€§æ˜ å°„ Qï¼Œ Kï¼Œ V
        Q = self.w_q(query)
        K = self.w_k(key)
        V = self.w_v(value)
        
        Q = split_heads(Q) # [batch_size, n_heads, seq_len, d_k]
        K = split_heads(K)
        V = split_heads(V)

        # attention
        context, attention = self.attention(Q, K, V, mask)
        # context: [batch_size, n_heads, seq_len, d_k]

        # merge multi-head
        context = concat_heads(context)

        output = self.w_o(context)
        output = self.dropout(output)

        return output, attention

# 2, æ‹†åˆ†æˆå¤šå¤´, d_model -> n_heads * d_k
def split_heads(x):
    # [batch_size, seq_len, d_model] -> [batch_size, seq_len, n_heads, d_k]
    # [batch_size, seq_len, n_heads, d_k] -> [batch_size, n_heads, seq_len, d_k]
    return x.view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)

# 5. merge multi-head: [B, H, L, d_k] -> [B, L, H, d_k] -> [B, L, d_model]
def comcat_heads(x):
    # exchange L and H 
    x = x.transpose(1, 2)  # [B, L, H, d_k]
    # å†åˆå¹¶æœ€åä¸¤ç»´ï¼šH * d_k = d_model
    return x.contiguous().view(batch_size, -1, self.d_model)